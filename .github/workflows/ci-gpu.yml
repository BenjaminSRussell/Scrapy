name: GPU CI (self-hosted)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 8 * * *"  # daily 08:00 UTC

jobs:
  gpu-tests:
    runs-on: [ self-hosted, windows, gpu ]
    timeout-minutes: 90
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (CUDA runner)
        shell: pwsh
        run: |
          python -m pip install --upgrade pip
          if (Test-Path "requirements-gpu.txt") {
            pip install -r requirements-gpu.txt
          } elseif (Test-Path "requirements.txt") {
            pip install -r requirements.txt
          } else {
            pip install torch --index-url https://download.pytorch.org/whl/cu121
            pip install pytest pytest-asyncio pytest-cov pytest-xdist ruff transformers accelerate sentence-transformers spacy
          }

      - name: Verify CUDA
        shell: pwsh
        run: |
          python - << 'PY'
          import torch
          assert torch.cuda.is_available(), "CUDA not available on this runner"
          print("CUDA devices:", torch.cuda.device_count())
          print("Device 0:", torch.cuda.get_device_name(0))
          PY

      - name: GPU smoke (sentence-transformers on cuda:0)
        shell: pwsh
        run: |
          python - << 'PY'
          from transformers import AutoModel, AutoTokenizer
          tok = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
          model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2").to("cuda:0")
          print("OK - model device:", next(model.parameters()).device)
          PY
