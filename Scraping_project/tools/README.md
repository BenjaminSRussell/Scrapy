# Post-Crawl Seed URL Update Script

This script processes the `validation_output.jsonl` file generated by Stage 2 of the scraping pipeline to identify high-quality URLs and update the seed URL file (`uconn_urls.csv`).

## How it Works

The script reads the `validation_output.jsonl` file and filters for URLs that meet the following criteria:

- HTTP status code of 200 (OK)
- Content-Type header of `text/html`
- The URL is not a `mailto` link
- The URL has been successfully validated at least 3 times (this is configurable)

The script then adds the new, high-quality URLs to the `uconn_urls.csv` file, avoiding duplicates.

## How to Run the Script

1.  **Navigate to the `tools` directory:**

    ```bash
    cd Scraping_project/tools
    ```

2.  **Run the script:**

    ```bash
    python update_seeds.py
    ```

3.  **Review the output:**

The script will print the number of new URLs added to the seed file and the location of the updated file.

## Configuration

You can configure the minimum number of successful validations required for a URL to be considered "high-quality" by editing the `min_successful_validations` variable in the `update_seeds.py` script.
